deepseek的爆火引起了我搭载本地AI模型的兴趣，因为我成功用AI绘画跑过1920x1920的图片，所以我对自己电脑的配置还是有信心的，毕竟画画总是要比写字难的，在网上查到的资料也基本是这个观点。然而，事实上却有所出入。聊天模型往往更大，功能也更复杂，相同的体量下，表现不如AI绘画模型。而太大的体量对电脑来说负担太大，哪怕.gguf格式的模型是经过压缩的，对显存要求小于GPTQ格式的，效果也不行，2G的模型虽然不太聪明，但是回复快，5G的，速度就明显慢下来了，哪怕回答得更好。如果真的有需要问AI的，还是用线上的，离线的效果一般，回复的内容短，且呆板。除非配置足够好，直接整一个几十G的，不然还是线上的用着舒服。

捣鼓了这么久，我并非一无所获，多少积累了一些经验。

本地配置模型用得比较多的是ollama和LM Studio，ollama虽然没有UI界面，但是同一个模型，它跑起来更快，对配置不够的用户比较友好，而且，UI界面也不难配置，网上流行的是Open-WebUI和Chatbox,不过考虑到ollama自己就四个多G了，而且为了运行方便，它往往放在C盘，为了不给系统盘太大的负担，我又找了一个比较冷门的配置方法

![屏幕截图 2025-02-08 203045](https://ai-work-try.oss-cn-guangzhou.aliyuncs.com/file/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202025-02-08%20203045.png)

![屏幕截图 2025-02-08 203312](https://ai-work-try.oss-cn-guangzhou.aliyuncs.com/file/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202025-02-08%20203312.png)

在这里我用的模型是在抱脸网上找到的，专门用来扩写小说的，看起来不怎么聪明。为了使得自己找到的模型可以由ollama运行，我费了不少功夫，终于找到相关的代码，出于对作者的尊重，我就不写在这了，把链接放在这里：[本地GGUF大模型导入ollama_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1BRNpeAE9T/?spm_id_from=333.1391.0.0&vd_source=685c09d8aef98d0a6519616a82068938)



至于LM Studio，自带UI比较好操作，方便，功能也多，只不过运行起来速度不如ollama。如果配置足够，那使用这个会更舒服。



模型的话，还是ollama官网的比较聪明，但是种类相对少，其他网站的，同等体量下的表现不行。ollama下载模型如果比较慢，可以Ctrl+c，打断之后再使用下载指令，速度就会快起来了。